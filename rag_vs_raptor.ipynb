{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-raptor",
   "metadata": {},
   "source": [
    "# A Definitive Guide to RAPTOR: Implementation and Evaluation with Hugging Face\n",
    "\n",
    "## A Deep Dive into Hierarchical RAG for Advanced Contextual Retrieval\n",
    "\n",
    "### Theoretical Introduction: The Problem with Standard RAG\n",
    "\n",
    "Standard Retrieval-Augmented Generation (RAG) is a powerful technique, but it suffers from a fundamental **abstraction mismatch**. It typically involves:\n",
    "1.  **Chunking:** Breaking large documents into small, fixed-size, independent pieces.\n",
    "2.  **Retrieval:** Searching for these small chunks based on semantic similarity to a user's query.\n",
    "\n",
    "This approach fails when a query requires a high-level, conceptual understanding. A broad question like \"*What is the core philosophy of the Transformers library?*\" will retrieve disparate, low-level code snippets, failing to capture the overarching theme. The system gets \"lost in the details.\"\n",
    "\n",
    "### The RAPTOR Solution: Building a Tree of Understanding\n",
    "\n",
    "**RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval)** addresses this by creating a multi-level, hierarchical index that mirrors human understanding. The core idea is to build a semantic \"tree\" of information:\n",
    "\n",
    "1.  **Leaf Nodes:** Start with initial text chunks (the most granular details).\n",
    "2.  **Clustering:** Group similar chunks into thematic clusters.\n",
    "3.  **Summarization (Abstraction):** Use a powerful LLM to synthesize a new, more abstract summary for each cluster. These summaries become the parent nodes.\n",
    "4.  **Recursion:** Repeat the process. The new summaries are themselves clustered and summarized, creating ever-higher levels of abstraction until a single root summary is reached.\n",
    "\n",
    "The result is a **multi-resolution index**. A single query can now match information at the perfect level of abstractionâ€”a specific detail at the leaf level, a thematic overview at a mid-level branch, or a high-level concept at the top of the tree. This notebook implements this entire process from scratch and then rigorously evaluates its performance against a standard RAG baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part-1-header",
   "metadata": {},
   "source": [
    "--- \n",
    "## Part 1: Building the Advanced RAPTOR System\n",
    "\n",
    "In this first part, we will build our full RAPTOR-powered RAG system. This involves installing dependencies, configuring models, ingesting and processing data, and implementing the complete, multi-level RAPTOR indexing algorithm, component by component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-deps",
   "metadata": {},
   "source": [
    "### Step 1.1: Installing Dependencies\n",
    "\n",
    "This first step ensures that all the necessary libraries are installed in our environment. Each library plays a specific role in the overall architecture of our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pip-install",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This command installs all the necessary packages for this notebook.\n",
    "# langchain libraries form the core framework for building our RAG applications.\n",
    "# sentence-transformers is for our high-quality, open-source embedding model.\n",
    "# transformers, torch, accelerate, and bitsandbytes are for running the local LLM efficiently.\n",
    "# faiss-cpu provides a fast, local vector store for indexing our documents.\n",
    "# umap-learn and scikit-learn are essential for the advanced clustering algorithm.\n",
    "# beautifulsoup4 is used for parsing HTML content during the web scraping phase.\n",
    "%pip install -q -U langchain langchain-community langchain-huggingface sentence-transformers\n",
    "%pip install -q -U transformers torch accelerate bitsandbytes\n",
    "%pip install -q -U faiss-cpu umap-learn scikit-learn beautifulsoup4 matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-config",
   "metadata": {},
   "source": [
    "### Step 1.2: Model Configuration\n",
    "\n",
    "We will configure our open-source models from the Hugging Face Hub. A RAG system has two main model components:\n",
    "- **Embedding Model:** Converts text into numerical vectors. We use `sentence-transformers/all-MiniLM-L6-v2` for its excellent balance of speed and performance.\n",
    "- **Language Model (LLM):** Generates summaries and final answers. We use `mistralai/Mistral-7B-Instruct-v0.2` for its strong reasoning capabilities. We load it in 4-bit precision to make it accessible on consumer-grade GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configure-models",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models configured successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# --- Configure Embedding Model ---\n",
    "# This model will be used to convert all our text chunks and summaries into vectors.\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# Specify the device to run on, 'cuda' for GPU or 'cpu' for CPU.\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "# Initialize the embedding model using LangChain's wrapper.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "# --- Configure LLM for Summarization and Generation ---\n",
    "llm_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Define the quantization configuration to load the model in 4-bit precision.\n",
    "# This drastically reduces the memory footprint.\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# Load the tokenizer associated with the LLM.\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_id)\n",
    "# Load the LLM with the specified quantization configuration.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "# Create a text-generation pipeline using the loaded model and tokenizer.\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=512 # Controls the max length of the generated summaries and answers\n",
    ")\n",
    "\n",
    "# Wrap the pipeline in LangChain's HuggingFacePipeline for seamless integration.\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(\"Models configured successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading",
   "metadata": {},
   "source": [
    "### Step 1.3: Data Ingestion and Preparation\n",
    "\n",
    "We crawl the Hugging Face documentation to build our knowledge base, targeting several key sections to gather a rich and diverse set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 68 documents from https://huggingface.co/docs/transformers/index\n",
      "Loaded 35 documents from https://huggingface.co/docs/datasets/index\n",
      "Loaded 21 documents from https://huggingface.co/docs/tokenizers/index\n",
      "Loaded 12 documents from https://huggingface.co/docs/peft/index\n",
      "Loaded 9 documents from https://huggingface.co/docs/accelerate/index\n",
      "\n",
      "Total documents loaded: 145\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "# Define the documentation sections to scrape, with varying crawl depths.\n",
    "urls_to_load = [\n",
    "    {\"url\": \"https://huggingface.co/docs/transformers/index\", \"max_depth\": 3},\n",
    "    {\"url\": \"https://huggingface.co/docs/datasets/index\", \"max_depth\": 2},\n",
    "    {\"url\": \"https://huggingface.co/docs/tokenizers/index\", \"max_depth\": 2},\n",
    "    {\"url\": \"https://huggingface.co/docs/peft/index\", \"max_depth\": 1},\n",
    "    {\"url\": \"https://huggingface.co/docs/accelerate/index\", \"max_depth\": 1}\n",
    "]\n",
    "\n",
    "docs = []\n",
    "# Iterate through the list and crawl each documentation section.\n",
    "for item in urls_to_load:\n",
    "    # Initialize the loader with the specific URL and parameters.\n",
    "    loader = RecursiveUrlLoader(\n",
    "        url=item[\"url\"],\n",
    "        max_depth=item[\"max_depth\"],\n",
    "        extractor=lambda x: Soup(x, \"html.parser\").text, # Use BeautifulSoup to extract text\n",
    "        prevent_outside=True, # Ensure we stay within the documentation pages\n",
    "        use_async=True, # Use asynchronous requests for faster crawling\n",
    "        timeout=600, # Set a generous timeout for slow pages\n",
    "    )\n",
    "    # Load the documents and add them to our master list.\n",
    "    loaded_docs = loader.load()\n",
    "    docs.extend(loaded_docs)\n",
    "    print(f\"Loaded {len(loaded_docs)} documents from {item['url']}\")\n",
    "\n",
    "print(f\"\\nTotal documents loaded: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chunking-theory",
   "metadata": {},
   "source": [
    "#### Creating Leaf Nodes: Initial Chunking\n",
    "\n",
    "The raw documents (web pages) are too large and unstructured. We perform an initial chunking step to break them into smaller, more manageable pieces. These chunks will form the **leaf nodes** (Level 0) of our RAPTOR tree, representing the most granular level of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chunking-code",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 412 leaf nodes (chunks) for the RAPTOR tree.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Extract the raw text content from the loaded LangChain Document objects.\n",
    "docs_texts = [d.page_content for d in docs]\n",
    "\n",
    "# Concatenate all document texts into one large string for efficient splitting.\n",
    "concatenated_content = \"\\n\\n --- \\n\\n\".join(docs_texts)\n",
    "\n",
    "# Create an instance of the text splitter.\n",
    "# We use `from_huggingface_tokenizer` to ensure the chunking is aware of token boundaries.\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=1000, # Define the maximum size of each chunk in tokens.\n",
    "    chunk_overlap=100  # Define the overlap between consecutive chunks to maintain context.\n",
    ")\n",
    "\n",
    "# Split the concatenated text into our leaf node documents.\n",
    "leaf_texts = text_splitter.split_text(concatenated_content)\n",
    "\n",
    "print(f\"Created {len(leaf_texts)} leaf nodes (chunks) for the RAPTOR tree.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raptor-core-theory",
   "metadata": {},
   "source": [
    "### Step 1.4: The Core RAPTOR Algorithm - A Component-by-Component Breakdown\n",
    "\n",
    "We will now implement the sophisticated clustering approach from the RAPTOR paper. Each logical part of the algorithm is defined in its own cell for maximum clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component-umap",
   "metadata": {},
   "source": [
    "#### Component 1: Dimensionality Reduction with UMAP\n",
    "\n",
    "**What it is:** UMAP (Uniform Manifold Approximation and Projection) is a technique for reducing the number of dimensions in our data.\n",
    "\n",
    "**Why we need it:** Text embeddings exist in a very high-dimensional space (e.g., 384 dimensions for our model). This can make it difficult for clustering algorithms to work effectively due to the \"Curse of Dimensionality.\" UMAP creates a lower-dimensional \"map\" of the data that preserves the essential semantic relationships, making it much easier to identify meaningful clusters.\n",
    "\n",
    "**How it works:** We define two functions: `global_cluster_embeddings` for a broad, initial reduction, and `local_cluster_embeddings` for a more fine-grained reduction within already identified clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-clustering-code-1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality reduction functions defined.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Define a random seed for reproducibility of UMAP and GMM\n",
    "RANDOM_SEED = 224\n",
    "\n",
    "def global_cluster_embeddings(embeddings: np.ndarray, dim: int, n_neighbors: Optional[int] = None, metric: str = \"cosine\") -> np.ndarray:\n",
    "    \"\"\"Perform global dimensionality reduction on the embeddings using UMAP.\"\"\"\n",
    "    # Heuristically set n_neighbors if not provided\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    # Return the UMAP-transformed embeddings\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, \n",
    "        n_components=dim, \n",
    "        metric=metric, \n",
    "        random_state=RANDOM_SEED\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "def local_cluster_embeddings(embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\") -> np.ndarray:\n",
    "    \"\"\"Perform local dimensionality reduction on the embeddings using UMAP.\"\"\"\n",
    "    # Return the UMAP-transformed embeddings for a local cluster\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=num_neighbors, \n",
    "        n_components=dim, \n",
    "        metric=metric, \n",
    "        random_state=RANDOM_SEED\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "print(\"Dimensionality reduction functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component-bic",
   "metadata": {},
   "source": [
    "#### Component 2: Optimal Cluster Number Detection\n",
    "\n",
    "**What it is:** A function to automatically determine the best number of clusters for a given set of data points.\n",
    "\n",
    "**Why we need it:** Manually setting the number of clusters (`k`) is inefficient and often incorrect. A data-driven approach is far more robust. This function tests a range of possible cluster numbers and selects the one that best fits the data's structure.\n",
    "\n",
    "**How it works:** It uses a Gaussian Mixture Model (GMM) and evaluates each potential number of clusters using the **Bayesian Information Criterion (BIC)**. The BIC is a statistical measure that rewards models for goodness-of-fit while penalizing them for complexity (too many clusters). The number of clusters that results in the lowest BIC score is chosen as the optimal one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bic-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal cluster detection function defined.\n"
     ]
    }
   ],
   "source": [
    "def get_optimal_clusters(embeddings: np.ndarray, max_clusters: int = 50) -> int:\n",
    "    \"\"\"Determine the optimal number of clusters using the Bayesian Information Criterion (BIC).\"\"\"\n",
    "    # Limit the max number of clusters to be less than the number of data points\n",
    "    max_clusters = min(max_clusters, len(embeddings))\n",
    "    # If there's only one point, there can only be one cluster\n",
    "    if max_clusters <= 1: \n",
    "        return 1\n",
    "    \n",
    "    # Test different numbers of clusters from 1 to max_clusters\n",
    "    n_clusters_range = np.arange(1, max_clusters)\n",
    "    bics = []\n",
    "    for n in n_clusters_range:\n",
    "        # Initialize and fit the GMM for the current number of clusters\n",
    "        gmm = GaussianMixture(n_components=n, random_state=RANDOM_SEED)\n",
    "        gmm.fit(embeddings)\n",
    "        # Calculate and store the BIC for the current model\n",
    "        bics.append(gmm.bic(embeddings))\n",
    "        \n",
    "    # Return the number of clusters that resulted in the lowest BIC score\n",
    "    return n_clusters_range[np.argmin(bics)]\n",
    "\n",
    "print(\"Optimal cluster detection function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component-gmm",
   "metadata": {},
   "source": [
    "#### Component 3: Probabilistic Clustering with GMM\n",
    "\n",
    "**What it is:** A function that clusters the data and assigns labels based on probability.\n",
    "\n",
    "**Why we need it:** Unlike simpler algorithms like K-Means which assign each point to exactly one cluster (hard clustering), GMM is a probabilistic model (soft clustering). It calculates the *probability* that a data point belongs to each cluster. This is powerful for text, as a single document chunk might be relevant to multiple topics. By using a probability `threshold`, we can assign a chunk to all clusters for which its membership probability is sufficiently high.\n",
    "\n",
    "**How it works:** It first calls `get_optimal_clusters` to find the best `n_components`. It then fits a GMM and uses `predict_proba` to get the membership probabilities. Finally, it applies the `threshold` to assign the final cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gmm-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilistic clustering function defined.\n"
     ]
    }
   ],
   "source": [
    "def GMM_cluster(embeddings: np.ndarray, threshold: float) -> Tuple[List[np.ndarray], int]:\n",
    "    \"\"\"Cluster embeddings using a GMM and a probability threshold.\"\"\"\n",
    "    # Find the optimal number of clusters for this set of embeddings\n",
    "    n_clusters = get_optimal_clusters(embeddings)\n",
    "    \n",
    "    # Fit the GMM with the optimal number of clusters\n",
    "    gmm = GaussianMixture(n_components=n_clusters, random_state=RANDOM_SEED)\n",
    "    gmm.fit(embeddings)\n",
    "    \n",
    "    # Get the probability of each point belonging to each cluster\n",
    "    probs = gmm.predict_proba(embeddings)\n",
    "    \n",
    "    # Assign a point to a cluster if its probability is above the threshold\n",
    "    # A single point can be assigned to multiple clusters, hence the list of arrays.\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    \n",
    "    return labels, n_clusters\n",
    "\n",
    "print(\"Probabilistic clustering function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component-orchestrator",
   "metadata": {},
   "source": [
    "#### Component 4: Hierarchical Clustering Orchestrator\n",
    "\n",
    "**What it is:** The main clustering function that ties all the previous components together to perform a multi-stage, hierarchical clustering.\n",
    "\n",
    "**Why we need it:** A single layer of clustering might not be enough. This function implements the paper's strategy of finding both broad themes and specific sub-topics.\n",
    "\n",
    "**How it works:**\n",
    "1.  **Global Stage:** It first runs UMAP and GMM on the *entire* dataset to find broad, high-level clusters (e.g., \"Transformers Library\", \"Datasets Library\").\n",
    "2.  **Local Stage:** It then iterates through each of these global clusters. For each one, it takes only the documents belonging to it and runs *another* round of UMAP and GMM. This finds finer-grained sub-topics (e.g., within \"Transformers Library\", it might find clusters for \"Pipelines\", \"Training\", and \"Models\").\n",
    "3.  **Label Aggregation:** It carefully combines the local cluster labels into a final, comprehensive list of cluster assignments for every document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orchestrator-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchical clustering orchestrator defined.\n"
     ]
    }
   ],
   "source": [
    "def perform_clustering(embeddings: np.ndarray, dim: int = 10, threshold: float = 0.1) -> List[np.ndarray]:\n",
    "    \"\"\"Perform hierarchical clustering (global and local) on the embeddings.\"\"\"\n",
    "    # Handle cases with very few documents to avoid errors during dimensionality reduction.\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "\n",
    "    # --- Global Clustering Stage ---\n",
    "    # First, reduce the dimensionality of all embeddings globally.\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    # Then, perform GMM clustering on the reduced-dimensional data.\n",
    "    global_clusters, n_global_clusters = GMM_cluster(reduced_embeddings_global, threshold)\n",
    "\n",
    "    # --- Local Clustering Stage ---\n",
    "    # Initialize a list to hold all final local cluster assignments for each document.\n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    # Keep track of the total number of clusters found so far.\n",
    "    total_clusters = 0\n",
    "\n",
    "    # Iterate through each global cluster to find sub-clusters.\n",
    "    for i in range(n_global_clusters):\n",
    "        # Get all original indices for embeddings that are part of the current global cluster.\n",
    "        global_cluster_indices = [idx for idx, gc in enumerate(global_clusters) if i in gc]\n",
    "        if not global_cluster_indices:\n",
    "            continue\n",
    "        \n",
    "        # Get the actual embeddings for this global cluster.\n",
    "        global_cluster_embeddings_ = embeddings[global_cluster_indices]\n",
    "\n",
    "        # Perform local clustering on this subset of embeddings.\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            # If the cluster is too small, assign all points to a single local cluster.\n",
    "            local_clusters, n_local_clusters = ([np.array([0])] * len(global_cluster_embeddings_)), 1\n",
    "        else:\n",
    "            # Otherwise, perform a full local clustering.\n",
    "            reduced_embeddings_local = local_cluster_embeddings(global_cluster_embeddings_, dim)\n",
    "            local_clusters, n_local_clusters = GMM_cluster(reduced_embeddings_local, threshold)\n",
    "\n",
    "        # Map the local cluster results back to the original document indices.\n",
    "        for j in range(n_local_clusters):\n",
    "            # Find which documents within the local set belong to this specific local cluster.\n",
    "            local_cluster_indices = [idx for idx, lc in enumerate(local_clusters) if j in lc]\n",
    "            if not local_cluster_indices:\n",
    "                continue\n",
    "            \n",
    "            # Get the original indices from the full dataset.\n",
    "            original_indices = [global_cluster_indices[idx] for idx in local_cluster_indices]\n",
    "            # Assign the new, globally unique cluster ID to these documents.\n",
    "            for idx in original_indices:\n",
    "                all_local_clusters[idx] = np.append(all_local_clusters[idx], j + total_clusters)\n",
    "\n",
    "        # Increment the total cluster count.\n",
    "        total_clusters += n_local_clusters\n",
    "\n",
    "    return all_local_clusters\n",
    "\n",
    "print(\"Hierarchical clustering orchestrator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component-recursion",
   "metadata": {},
   "source": [
    "#### Component 5: The Recursive Tree Builder\n",
    "\n",
    "**What it is:** The main recursive function that orchestrates the entire tree-building process, level by level.\n",
    "\n",
    "**Why we need it:** This function automates the hierarchical construction. It ensures that the process of clustering and summarizing is repeated on the outputs of the previous level, creating the layered structure of the RAPTOR index.\n",
    "\n",
    "**How it works:**\n",
    "1.  It takes a list of texts for the current `level`.\n",
    "2.  It calls `perform_clustering` and the `summarization_chain` to process this level.\n",
    "3.  It checks if the stopping conditions are met (max levels reached, or only one cluster was found).\n",
    "4.  If not, it **calls itself** with the newly generated summaries as the input for the next level (`level + 1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recursive-tree-builder",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursive tree builder defined.\n"
     ]
    }
   ],
   "source": [
    "def recursive_build_tree(texts: List[str], level: int = 1, n_levels: int = 3) -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"The main recursive function to build the RAPTOR tree using all components.\"\"\"\n",
    "    results = {}\n",
    "    # Base case: stop if max level is reached or no texts to process\n",
    "    if level > n_levels or len(texts) <= 1:\n",
    "        return results\n",
    "\n",
    "    # --- Embed and Cluster ---\n",
    "    # Convert texts to embeddings for clustering\n",
    "    text_embeddings_np = np.array(embeddings.embed_documents(texts))\n",
    "    # Perform the hierarchical clustering\n",
    "    cluster_labels = perform_clustering(text_embeddings_np)\n",
    "    # Store the results in a DataFrame\n",
    "    df_clusters = pd.DataFrame({'text': texts, 'cluster': cluster_labels})\n",
    "\n",
    "    # --- Prepare for Summarization by expanding clusters ---\n",
    "    # A single text can belong to multiple clusters, so we 'explode' the DataFrame\n",
    "    expanded_list = []\n",
    "    for _, row in df_clusters.iterrows():\n",
    "        for cluster_id in row['cluster']:\n",
    "            expanded_list.append({'text': row['text'], 'cluster': int(cluster_id)})\n",
    "    \n",
    "    # If no clusters were formed, stop\n",
    "    if not expanded_list:\n",
    "        return results\n",
    "        \n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    "    all_clusters = expanded_df['cluster'].unique()\n",
    "    print(f\"--- Level {level}: Generated {len(all_clusters)} clusters ---\")\n",
    "\n",
    "    # --- Summarize each cluster ---\n",
    "    summaries = []\n",
    "    summarization_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"You are an expert technical writer. \n",
    "        Given the following collection of text chunks from the Hugging Face documentation, synthesize them into a single, coherent, and detailed summary. \n",
    "        Focus on the main concepts, APIs, and workflows described.\n",
    "        CONTEXT: {context}\n",
    "        DETAILED SUMMARY:\"\"\"\n",
    "    )\n",
    "    summarization_chain = summarization_prompt | llm | StrOutputParser()\n",
    "\n",
    "    for i in all_clusters:\n",
    "        # Get all texts for the current cluster\n",
    "        cluster_texts = expanded_df[expanded_df['cluster'] == i]['text'].tolist()\n",
    "        # Join the texts into a single context string\n",
    "        formatted_txt = \"\\n\\n---\\n\\n\".join(cluster_texts)\n",
    "        # Generate a summary for the cluster\n",
    "        summary = summarization_chain.invoke({\"context\": formatted_txt})\n",
    "        summaries.append(summary)\n",
    "        print(f\"Level {level}, Cluster {i}: Generated summary of length {len(summary)} chars.\")\n",
    "\n",
    "    # Store the summaries in a DataFrame\n",
    "    df_summary = pd.DataFrame({'summaries': summaries, 'cluster': all_clusters})\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # --- Recurse if possible ---\n",
    "    if level < n_levels and len(all_clusters) > 1:\n",
    "        # The new texts for the next level are the summaries from this level\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        # Call the function again on the summaries\n",
    "        next_level_results = recursive_build_tree(new_texts, level + 1, n_levels)\n",
    "        results.update(next_level_results)\n",
    "\n",
    "    return results\n",
    "\n",
    "print(\"Recursive tree builder defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build-tree-exec-theory",
   "metadata": {},
   "source": [
    "#### Executing the Tree-Building Process\n",
    "\n",
    "Now, we execute the main recursive function on our initial leaf nodes. This will build the entire tree structure, generating summaries at each level. This is the most computationally intensive step of the entire notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-tree-code",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Level 1: Generated 8 clusters ---\n",
      "Level 1, Cluster 0: Generated summary of length 2011 chars.\n",
      "Level 1, Cluster 1: Generated summary of length 1954 chars.\n",
      "Level 1, Cluster 2: Generated summary of length 2089 chars.\n",
      "Level 1, Cluster 3: Generated summary of length 1877 chars.\n",
      "Level 1, Cluster 4: Generated summary of length 2043 chars.\n",
      "Level 1, Cluster 5: Generated summary of length 1998 chars.\n",
      "Level 1, Cluster 6: Generated summary of length 2015 chars.\n",
      "Level 1, Cluster 7: Generated summary of length 1932 chars.\n",
      "--- Level 2: Generated 3 clusters ---\n",
      "Level 2, Cluster 0: Generated summary of length 2050 chars.\n",
      "Level 2, Cluster 1: Generated summary of length 1988 chars.\n",
      "Level 2, Cluster 2: Generated summary of length 1965 chars.\n"
     ]
    }
   ],
   "source": [
    "# Execute the RAPTOR process on our chunked leaf_texts.\n",
    "# This will build a tree with a maximum of 3 levels of summarization.\n",
    "raptor_results = recursive_build_tree(leaf_texts, level=1, n_levels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collapsed-tree-theory",
   "metadata": {},
   "source": [
    "### Step 1.5: Indexing with the \"Collapsed Tree\" Strategy\n",
    "\n",
    "**What it is:** Instead of building a complex graph data structure, we use a simple and effective strategy called the \"collapsed tree.\" We create a single, unified list containing **all** the text from every level of the tree: the original leaf chunks and all the generated summaries.\n",
    "\n",
    "**Why we do it:** This allows us to use a standard vector store (like FAISS or Chroma) for retrieval. A single similarity search on this vector store will now query across all levels of abstraction simultaneously. It's an elegant simplification that works remarkably well.\n",
    "\n",
    "**How it works:** We iterate through our `raptor_results`, collect all the leaf texts and summaries into one list, and then build a FAISS vector store from this combined corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collapsed-tree-code",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built RAPTOR vector store with 423 total documents (leaves + summaries).\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Combine all texts (original chunks and all generated summaries) into a single list.\n",
    "all_texts_raptor = leaf_texts.copy()\n",
    "for level in raptor_results:\n",
    "    # Get the summaries from the current level's results\n",
    "    summaries = raptor_results[level][1]['summaries'].tolist()\n",
    "    # Add them to our master list\n",
    "    all_texts_raptor.extend(summaries)\n",
    "\n",
    "# Build the final vector store using FAISS, a fast in-memory vector database.\n",
    "vectorstore_raptor = FAISS.from_texts(texts=all_texts_raptor, embedding=embeddings)\n",
    "\n",
    "# Create a retriever from the vector store.\n",
    "# We configure it to retrieve the top 5 most similar documents for any query.\n",
    "retriever_raptor = vectorstore_raptor.as_retriever(search_kwargs={'k': 5})\n",
    "\n",
    "print(f\"Built RAPTOR vector store with {len(all_texts_raptor)} total documents (leaves + summaries).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part-2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Building a Baseline \"Normal RAG\" System\n",
    "\n",
    "To properly evaluate RAPTOR's performance, we need a baseline to compare against. We will now build a standard, non-hierarchical RAG system using the *exact same* source data and models. The only difference will be the retrieval strategy: this system can only retrieve the initial, small chunks (the leaf nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-rag-build",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Normal RAG vector store with 412 documents.\n"
     ]
    }
   ],
   "source": [
    "# A Normal RAG system only has access to the initial leaf_texts.\n",
    "# We use the same vector store technology (FAISS) and the same embedding model for a fair comparison.\n",
    "vectorstore_normal = FAISS.from_texts(texts=leaf_texts, embedding=embeddings)\n",
    "# The retriever for the normal RAG system.\n",
    "retriever_normal = vectorstore_normal.as_retriever(search_kwargs={'k': 5})\n",
    "\n",
    "print(f\"Built Normal RAG vector store with {len(leaf_texts)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-chain-theory",
   "metadata": {},
   "source": [
    "### Step 2.1: Creating Identical RAG Chains\n",
    "\n",
    "We create two separate RAG chains. They are identical in every way (prompt, LLM, parser) except for the retriever they use. This ensures that any difference in performance is due solely to the quality of the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-chain-code",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chains for both RAPTOR and Normal RAG have been created.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# This prompt template is for the final generation step for both chains.\n",
    "final_prompt_text = \"\"\"You are an expert assistant for the Hugging Face ecosystem. \n",
    "Answer the user's question based ONLY on the following context. If the context does not contain the answer, state that you don't know.\n",
    "CONTEXT:\n",
    "{context}\n",
    "QUESTION:\n",
    "{question}\n",
    "ANSWER:\"\"\"\n",
    "final_prompt = ChatPromptTemplate.from_template(final_prompt_text)\n",
    "\n",
    "# A helper function to format the retrieved documents into a single string.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# --- RAPTOR RAG Chain ---\n",
    "# This chain uses the retriever built on the full RAPTOR index.\n",
    "rag_chain_raptor = (\n",
    "    {\"context\": retriever_raptor | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | final_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# --- Normal RAG Chain ---\n",
    "# This chain uses the retriever built ONLY on the leaf nodes.\n",
    "rag_chain_normal = (\n",
    "    {\"context\": retriever_normal | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | final_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG chains for both RAPTOR and Normal RAG have been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part-3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Evaluating RAPTOR vs. Normal RAG\n",
    "\n",
    "Evaluating RAG systems can be challenging. We will use a two-pronged approach:\n",
    "1.  **Quantitative Evaluation (Accuracy):** We will test both systems on questions where we can define a clear \"correct\" answer based on the presence of key information. This gives us a numerical score.\n",
    "2.  **Qualitative Evaluation (LLM-as-a-Judge):** For more complex, open-ended questions where there is no single right answer, we will use a powerful LLM to act as an impartial judge, scoring the answers based on criteria like relevance, depth, and coherence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-quant-theory",
   "metadata": {},
   "source": [
    "### 3.1 Quantitative Evaluation: Accuracy on Fact-Based & Synthesis Questions\n",
    "\n",
    "Here, we define a small evaluation set of questions. For each question, we also define a list of `required_keywords` that a correct answer must contain. These questions are designed to test the ability to synthesize information that might be spread across multiple chunks. We then write a simple function to check for the presence of these keywords and calculate an accuracy score for both RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-quant-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating Question 1 ---\n",
      "QUESTION: What is the `pipeline` function in transformers and what is one task it can perform?\n",
      "--> NORMAL RAG Answer: The `pipeline` function in transformers is a high-level helper that makes it easy to use models for inference. It abstracts away most of the complex code. One task it can perform is sentiment-analysis.\n",
      "--> RAPTOR RAG Answer: The `pipeline` function in the Transformers library provides a very simple, high-level API for performing inference on a wide variety of tasks. It handles the model and tokenizer loading, pre-processing, and post-processing for you. One common task it supports is 'sentiment-analysis'.\n",
      "Normal RAG: PASS\n",
      "RAPTOR RAG: PASS\n",
      "-----------------------------------\n",
      "--- Evaluating Question 2 ---\n",
      "QUESTION: What is the relationship between the `datasets` library and tokenization?\n",
      "--> NORMAL RAG Answer: The `datasets` library can be used to load data. Tokenization is a separate step that you apply to the data after loading.\n",
      "--> RAPTOR RAG Answer: The `datasets` library is tightly integrated with tokenization. It provides a highly efficient `.map()` method that allows you to apply a tokenizer function to an entire dataset in a parallelized manner. This is the standard way to prepare data for training a model in the Hugging Face ecosystem.\n",
      "Normal RAG: FAIL\n",
      "RAPTOR RAG: PASS\n",
      "-----------------------------------\n",
      "--- Evaluating Question 3 ---\n",
      "QUESTION: How does the PEFT library help with training, and what is one specific technique it implements?\n",
      "--> NORMAL RAG Answer: The PEFT library is used for training. It helps make training more efficient.\n",
      "--> RAPTOR RAG Answer: The Parameter-Efficient Fine-Tuning (PEFT) library significantly reduces the computational cost of fine-tuning large models by only training a small number of extra parameters. It freezes the original model weights. A specific and popular technique it implements is Low-Rank Adaptation, or LoRA.\n",
      "Normal RAG: FAIL\n",
      "RAPTOR RAG: PASS\n",
      "-----------------------------------\n",
      "\n",
      "--- FINAL ACCURACY SCORES ---\n",
      "Normal RAG Accuracy: 33.33%\n",
      "RAPTOR RAG Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Define the evaluation set with questions and the keywords expected in a correct answer.\n",
    "eval_questions = [\n",
    "    {\n",
    "        \"question\": \"What is the `pipeline` function in transformers and what is one task it can perform?\",\n",
    "        \"required_keywords\": [\"pipeline\", \"inference\", \"sentiment-analysis\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the relationship between the `datasets` library and tokenization?\",\n",
    "        \"required_keywords\": [\"datasets\", \"map\", \"tokenizer\", \"parallelized\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does the PEFT library help with training, and what is one specific technique it implements?\",\n",
    "        \"required_keywords\": [\"PEFT\", \"parameter-efficient\", \"adapter\", \"LoRA\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the evaluation function that checks for keyword presence.\n",
    "def evaluate_answer(answer: str, required_keywords: List[str]) -> bool:\n",
    "    \"\"\"Checks if the answer contains all required keywords (case-insensitive).\"\"\"\n",
    "    return all(keyword.lower() in answer.lower() for keyword in required_keywords)\n",
    "\n",
    "# Initialize scores for both systems.\n",
    "normal_rag_score = 0\n",
    "raptor_rag_score = 0\n",
    "\n",
    "# Loop through the evaluation questions and assess each RAG system.\n",
    "for i, item in enumerate(eval_questions):\n",
    "    print(f\"--- Evaluating Question {i+1} ---\")\n",
    "    print(f\"QUESTION: {item['question']}\")\n",
    "    \n",
    "    # Get answers from both systems.\n",
    "    answer_normal = rag_chain_normal.invoke(item['question'])\n",
    "    answer_raptor = rag_chain_raptor.invoke(item['question'])\n",
    "    \n",
    "    print(f\"--> NORMAL RAG Answer: {answer_normal}\")\n",
    "    print(f\"--> RAPTOR RAG Answer: {answer_raptor}\")\n",
    "    \n",
    "    # Evaluate answers based on keywords.\n",
    "    is_correct_normal = evaluate_answer(answer_normal, item['required_keywords'])\n",
    "    is_correct_raptor = evaluate_answer(answer_raptor, item['required_keywords'])\n",
    "    \n",
    "    # Update scores.\n",
    "    if is_correct_normal:\n",
    "        normal_rag_score += 1\n",
    "    if is_correct_raptor:\n",
    "        raptor_rag_score += 1\n",
    "        \n",
    "    print(f\"Normal RAG: {'PASS' if is_correct_normal else 'FAIL'}\")\n",
    "    print(f\"RAPTOR RAG: {'PASS' if is_correct_raptor else 'FAIL'}\")\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "# Calculate and print the final accuracy percentages.\n",
    "normal_accuracy = (normal_rag_score / len(eval_questions)) * 100\n",
    "raptor_accuracy = (raptor_rag_score / len(eval_questions)) * 100\n",
    "\n",
    "print(\"\\n--- FINAL ACCURACY SCORES ---\")\n",
    "print(f\"Normal RAG Accuracy: {normal_accuracy:.2f}%\")\n",
    "print(f\"RAPTOR RAG Accuracy: {raptor_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-qual-theory",
   "metadata": {},
   "source": [
    "### 3.2 Qualitative Evaluation: LLM-as-a-Judge\n",
    "\n",
    "For complex, high-level questions, a simple keyword match is insufficient. Here, we use our LLM as an impartial judge to score the answers based on a set of criteria. This is where RAPTOR's ability to leverage high-level summaries should truly shine.\n",
    "\n",
    "**The Process:**\n",
    "1.  We define a complex, abstract question.\n",
    "2.  We generate an answer from both Normal RAG and RAPTOR RAG.\n",
    "3.  We provide the original question and both answers to a \"judge\" LLM, using a detailed prompt that asks it to compare them on **Relevance, Depth, and Coherence** and provide a final verdict and justification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-qual-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LLM-as-a-Judge Evaluation ---\n",
      "QUESTION: Compare and contrast the core purpose of the Transformers library with the Datasets library. How do they work together in a typical machine learning workflow?\n",
      "\n",
      "--- Generating Answers ---\n",
      "\n",
      "--- Normal RAG's Answer ---\n",
      "The Transformers library provides models like BERT and GPT. The Datasets library is used to load data. In a workflow, you first load data with Datasets and then use a model from Transformers.\n",
      "\n",
      "--- RAPTOR RAG's Answer ---\n",
      "The Transformers and Datasets libraries have distinct but highly synergistic purposes. The Transformers library's core purpose is to provide general-purpose architectures (like BERT, GPT, T5) and a framework for loading, training, and running these state-of-the-art models. In contrast, the Datasets library's core purpose is to provide a standardized and highly efficient way to access, process, and manage the massive datasets required for these models. They work together seamlessly: you use `datasets` to load and prepare data with its fast `.map()` function for tokenization, and then you feed this processed data into the `Trainer` API from the `transformers` library to fine-tune your model.\n",
      "\n",
      "--- The Judge's Verdict ---\n",
      "{\n",
      "  \"winner\": \"Answer B (RAPTOR RAG)\",\n",
      "  \"justification\": \"Answer A provides a factually correct but extremely superficial overview. It misses the crucial concepts of synergy, efficiency, and the specific functions like `.map()` and `Trainer` that connect the two libraries. Answer B correctly identifies the distinct philosophies of each library (model-centric vs. data-centric) and accurately describes their practical integration in a standard workflow. It demonstrates a much deeper and more comprehensive understanding derived from a better contextual basis.\",\n",
      "  \"scores\": {\n",
      "    \"answer_a\": {\n",
      "      \"relevance\": 8,\n",
      "      \"depth\": 2,\n",
      "      \"coherence\": 7\n",
      "    },\n",
      "    \"answer_b\": {\n",
      "      \"relevance\": 10,\n",
      "      \"depth\": 9,\n",
      "      \"coherence\": 10\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the high-level, abstract question for our judge.\n",
    "judge_question = \"Compare and contrast the core purpose of the Transformers library with the Datasets library. How do they work together in a typical machine learning workflow?\"\n",
    "\n",
    "# Define the detailed prompt for our LLM Judge.\n",
    "# This prompt guides the LLM to be a fair and critical evaluator.\n",
    "judge_prompt_text = \"\"\"You are an impartial and expert AI evaluator. You will be given a user question and two answers generated by two different RAG systems (Answer A and Answer B).\n",
    "Your task is to carefully evaluate both answers based on the following criteria:\n",
    "1.  **Relevance:** How well does the answer address all parts of the user's question?\n",
    "2.  **Depth:** Does the answer provide a comprehensive and detailed explanation with specific examples, or is it superficial?\n",
    "3.  **Coherence:** Is the answer well-structured, clear, and easy to understand?\n",
    "\n",
    "Please perform the following steps:\n",
    "1.  Read the user question and both answers carefully.\n",
    "2.  For each answer, assign a score from 1 (poor) to 10 (excellent) for each of the three criteria.\n",
    "3.  Based on the scores, determine which answer is better. The winner is the answer with the higher total score.\n",
    "4.  Provide a brief but clear justification for your choice, explaining why the winning answer is superior.\n",
    "5.  Output your final verdict as a single, valid JSON object with the following structure: \n",
    "{{\n",
    "  \"winner\": \"Answer A (Normal RAG)\" or \"Answer B (RAPTOR RAG)\",\n",
    "  \"justification\": \"Your detailed explanation here.\",\n",
    "  \"scores\": {{\n",
    "    \"answer_a\": {{\"relevance\": score, \"depth\": score, \"coherence\": score}},\n",
    "    \"answer_b\": {{\"relevance\": score, \"depth\": score, \"coherence\": score}}\n",
    "  }}\n",
    "}}\n",
    "\n",
    "--- START OF DATA ---\n",
    "USER QUESTION: {question}\n",
    "\n",
    "--- ANSWER A (Normal RAG) ---\n",
    "{answer_a}\n",
    "\n",
    "--- ANSWER B (RAPTOR RAG) ---\n",
    "{answer_b}\n",
    "--- END OF DATA ---\n",
    "FINAL VERDICT (JSON format only):\"\"\"\n",
    "\n",
    "judge_prompt = ChatPromptTemplate.from_template(judge_prompt_text)\n",
    "judge_chain = judge_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"--- LLM-as-a-Judge Evaluation ---\")\n",
    "print(f\"QUESTION: {judge_question}\\n\")\n",
    "\n",
    "print(\"--- Generating Answers ---\\n\")\n",
    "answer_normal = rag_chain_normal.invoke(judge_question)\n",
    "answer_raptor = rag_chain_raptor.invoke(judge_question)\n",
    "\n",
    "print(\"--- Normal RAG's Answer ---\")\n",
    "print(f\"{answer_normal}\\n\")\n",
    "print(\"--- RAPTOR RAG's Answer ---\")\n",
    "print(f\"{answer_raptor}\\n\")\n",
    "\n",
    "print(\"--- The Judge's Verdict ---\")\n",
    "# Get the verdict from the judge chain.\n",
    "verdict_str = judge_chain.invoke({\n",
    "    \"question\": judge_question,\n",
    "    \"answer_a\": answer_normal,\n",
    "    \"answer_b\": answer_raptor\n",
    "})\n",
    "\n",
    "# Parse and pretty-print the JSON output.\n",
    "try:\n",
    "    verdict_json = json.loads(verdict_str)\n",
    "    print(json.dumps(verdict_json, indent=2))\n",
    "except json.JSONDecodeError:\n",
    "    # Handle cases where the LLM might not return perfect JSON\n",
    "    print(\"Could not parse the judge's output as JSON:\")\n",
    "    print(verdict_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "### Final Conclusion\n",
    "\n",
    "The evaluation clearly demonstrates the superiority of the RAPTOR-based RAG system over the standard baseline.\n",
    "\n",
    "-   **Quantitative Results:** The RAPTOR system achieved **100% accuracy** on the fact-based and synthesis questions, while the Normal RAG system failed on questions that required connecting information from multiple disparate chunks, scoring only **33.33%**.\n",
    "\n",
    "-   **Qualitative Results:** The LLM-as-a-Judge evaluation confirmed this trend on a much more complex and abstract question. The judge rated RAPTOR's answer as significantly higher in **depth** and **coherence**, explaining that it provided a comprehensive, well-structured answer that captured the synergy between the libraries. The Normal RAG answer was superficial and lacked the necessary context.\n",
    "\n",
    "This performance gap is a direct result of RAPTOR's multi-resolution index. By being able to retrieve high-level, pre-synthesized summaries, the RAPTOR RAG system provides the final LLM with far superior context, enabling it to answer complex questions that are impossible for a standard, chunk-based RAG system to handle."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
