{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-raptor",
   "metadata": {},
   "source": [
    "# End-to-End Implementation of RAPTOR with Hugging Face\n",
    "\n",
    "## A Deep Dive into Hierarchical RAG for Advanced Contextual Retrieval\n",
    "\n",
    "### Theoretical Introduction: The Problem with Standard RAG\n",
    "\n",
    "Standard Retrieval-Augmented Generation (RAG) is a powerful technique, but it suffers from a fundamental **abstraction mismatch**. It typically involves:\n",
    "1.  **Chunking:** Breaking large documents into small, fixed-size, independent pieces.\n",
    "2.  **Retrieval:** Searching for these small chunks based on semantic similarity to a user's query.\n",
    "\n",
    "This approach fails when a query requires a high-level, conceptual understanding. A broad question like \"*What is the core philosophy of the Transformers library?*\" will retrieve disparate, low-level code snippets, failing to capture the overarching theme. The system gets \"lost in the details.\"\n",
    "\n",
    "### The RAPTOR Solution: Building a Tree of Understanding\n",
    "\n",
    "**RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval)** addresses this by creating a multi-level, hierarchical index that mirrors human understanding. The core idea is to build a semantic \"tree\" of information:\n",
    "\n",
    "1.  **Leaf Nodes:** Start with initial text chunks (the most granular details).\n",
    "2.  **Clustering:** Group similar chunks into thematic clusters.\n",
    "3.  **Summarization (Abstraction):** Use a powerful LLM to synthesize a new, more abstract summary for each cluster. These summaries become the parent nodes.\n",
    "4.  **Recursion:** Repeat the process. The new summaries are themselves clustered and summarized, creating ever-higher levels of abstraction until a single root summary is reached.\n",
    "\n",
    "The result is a **multi-resolution index**. A single query can now match information at the perfect level of abstractionâ€”a specific detail at the leaf level, a thematic overview at a mid-level branch, or a high-level concept at the top of the tree. This notebook implements this entire process from scratch, using the advanced clustering techniques from the original paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-deps",
   "metadata": {},
   "source": [
    "### Step 1: Installing Dependencies\n",
    "\n",
    "First, we install all the necessary libraries. We'll use `transformers` and `sentence-transformers` for our Hugging Face models, `faiss-cpu` for efficient vector indexing, and `umap-learn` with `scikit-learn` for the core clustering logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pip-install",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This command installs all the necessary packages for this notebook.\n",
    "# langchain libraries form the core framework.\n",
    "# sentence-transformers is for our embedding model.\n",
    "# transformers, torch, accelerate, and bitsandbytes are for running the local LLM.\n",
    "# faiss-cpu provides a fast, local vector store.\n",
    "# umap-learn and scikit-learn are for the clustering algorithm.\n",
    "# beautifulsoup4 is used for parsing HTML during web scraping.\n",
    "%pip install -q -U langchain langchain-community langchain-huggingface sentence-transformers\n",
    "%pip install -q -U transformers torch accelerate bitsandbytes\n",
    "%pip install -q -U faiss-cpu umap-learn scikit-learn beautifulsoup4 matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-config",
   "metadata": {},
   "source": [
    "### Step 2: Model Configuration\n",
    "\n",
    "We will configure our models from the Hugging Face Hub. For this demonstration, we'll use:\n",
    "- **Embedding Model:** `sentence-transformers/all-MiniLM-L6-v2`. A small, fast, and effective model for creating sentence and paragraph embeddings.\n",
    "- **LLM for Summarization:** `mistralai/Mistral-7B-Instruct-v0.2`. A powerful yet manageable model for the summarization task. We'll load it in 4-bit precision using `bitsandbytes` to conserve memory and make it runnable on consumer-grade GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configure-models",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models configured successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "\n",
    "# --- Configure Embedding Model ---\n",
    "# We use a sentence-transformer model for creating high-quality embeddings.\n",
    "# 'all-MiniLM-L6-v2' is a great choice for its balance of speed and performance.\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"} # Or 'cpu' if you don't have a GPU\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "# --- Configure LLM for Summarization ---\n",
    "# We use Mistral-7B, a powerful open-source model.\n",
    "# To make it runnable on a single GPU, we load it in 4-bit precision.\n",
    "llm_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Configuration for 4-bit quantization to save memory\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# Load the tokenizer and the 4-bit quantized model\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "# Create a text-generation pipeline from the loaded model and tokenizer\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=512 # Controls the max length of the generated summaries\n",
    ")\n",
    "\n",
    "# Wrap the pipeline in LangChain's HuggingFacePipeline for seamless integration\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(\"Models configured successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading",
   "metadata": {},
   "source": [
    "### Step 3: Data Ingestion and Preparation\n",
    "\n",
    "We will crawl the Hugging Face documentation to build our knowledge base. We target several key sections with varying crawl depths to gather a rich and diverse set of documents. This mimics a real-world scenario where a knowledge base is built from multiple related sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 68 documents from https://huggingface.co/docs/transformers/index\n",
      "Loaded 35 documents from https://huggingface.co/docs/datasets/index\n",
      "Loaded 21 documents from https://huggingface.co/docs/tokenizers/index\n",
      "Loaded 12 documents from https://huggingface.co/docs/peft/index\n",
      "Loaded 9 documents from https://huggingface.co/docs/accelerate/index\n",
      "\n",
      "Total documents loaded: 145\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "# A list of starting URLs for our knowledge base, with different crawl depths\n",
    "urls_to_load = [\n",
    "    {\"url\": \"https://huggingface.co/docs/transformers/index\", \"max_depth\": 3},\n",
    "    {\"url\": \"https://huggingface.co/docs/datasets/index\", \"max_depth\": 2},\n",
    "    {\"url\": \"https://huggingface.co/docs/tokenizers/index\", \"max_depth\": 2},\n",
    "    {\"url\": \"https://huggingface.co/docs/peft/index\", \"max_depth\": 1},\n",
    "    {\"url\": \"https://huggingface.co/docs/accelerate/index\", \"max_depth\": 1}\n",
    "]\n",
    "\n",
    "docs = []\n",
    "# Iterate through the list and crawl each documentation section\n",
    "for item in urls_to_load:\n",
    "    # Initialize the loader with the specific URL and parameters\n",
    "    loader = RecursiveUrlLoader(\n",
    "        url=item[\"url\"],\n",
    "        max_depth=item[\"max_depth\"],\n",
    "        extractor=lambda x: Soup(x, \"html.parser\").text, # Extracts plain text from HTML\n",
    "        prevent_outside=True, # Prevents crawling outside the /docs domain\n",
    "        use_async=True, # Speeds up crawling with asynchronous requests\n",
    "        timeout=600, # Increases timeout to handle slow pages\n",
    "    )\n",
    "    # Load the documents and add them to our list\n",
    "    loaded_docs = loader.load()\n",
    "    docs.extend(loaded_docs)\n",
    "    print(f\"Loaded {len(loaded_docs)} documents from {item['url']}\")\n",
    "\n",
    "print(f\"\\nTotal documents loaded: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "token-counting-theory",
   "metadata": {},
   "source": [
    "#### Document Analysis: Token Counting\n",
    "\n",
    "Before we proceed, it's crucial to understand the size of our documents. We'll use the tokenizer from our chosen LLM (`Mistral-7B`) to accurately count the tokens. This analysis will justify the need for our initial chunking step to create the leaf nodes for RAPTOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "token-counter",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 145\n",
      "Total tokens in corpus: 312560\n",
      "Average tokens per document: 2155.59\n",
      "Min tokens in a document: 312\n",
      "Max tokens in a document: 12450\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We need a consistent way to count tokens, using the LLM's tokenizer is the most accurate method.\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Counts the number of tokens in a text using the configured tokenizer.\"\"\"\n",
    "    # Ensure text is not None and is a string\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# Extract the text content from the loaded LangChain Document objects\n",
    "docs_texts = [d.page_content for d in docs]\n",
    "\n",
    "# Calculate token counts for each document\n",
    "token_counts = [count_tokens(text) for text in docs_texts]\n",
    "\n",
    "# Print statistics to understand the document size distribution\n",
    "print(f\"Total documents: {len(docs_texts)}\")\n",
    "print(f\"Total tokens in corpus: {np.sum(token_counts)}\")\n",
    "print(f\"Average tokens per document: {np.mean(token_counts):.2f}\")\n",
    "print(f\"Min tokens in a document: {np.min(token_counts)}\")\n",
    "print(f\"Max tokens in a document: {np.max(token_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot-code-theory",
   "metadata": {},
   "source": [
    "#### Visualizing Document Lengths\n",
    "\n",
    "A histogram helps visualize the distribution of document lengths. The output (which is omitted here as requested) typically shows a long tail, with many small documents and a few very large ones. This confirms that many documents are too large for direct use in an LLM context and must be chunked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-code",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This code generates a histogram to visually inspect the token counts.\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(token_counts, bins=50, color='blue', alpha=0.7)\n",
    "plt.title('Distribution of Document Token Counts')\n",
    "plt.xlabel('Token Count')\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chunking-theory",
   "metadata": {},
   "source": [
    "#### Creating Leaf Nodes: Initial Chunking\n",
    "\n",
    "Our analysis shows many documents are too large. We now perform an initial chunking step. These chunks will form the **leaf nodes** of our RAPTOR tree. We choose a chunk size that is large enough to contain meaningful context (e.g., a full function definition with its docstring) but small enough to be a focused unit of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chunking-code",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 412 leaf nodes (chunks) for the RAPTOR tree.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Concatenate all document texts into one large string for efficient splitting\n",
    "concatenated_content = \"\\n\\n --- \\n\\n\".join(docs_texts)\n",
    "\n",
    "# Create the text splitter, using the LLM's tokenizer for accurate splitting\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=1000, # The max number of tokens in a chunk\n",
    "    chunk_overlap=100  # The number of tokens to overlap between chunks\n",
    ")\n",
    "\n",
    "# Split the text into chunks, which will be our leaf nodes\n",
    "leaf_texts = text_splitter.split_text(concatenated_content)\n",
    "\n",
    "print(f\"Created {len(leaf_texts)} leaf nodes (chunks) for the RAPTOR tree.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raptor-core-theory",
   "metadata": {},
   "source": [
    "### Step 4: The Core RAPTOR Algorithm - A Component-by-Component Breakdown\n",
    "\n",
    "We will now implement the sophisticated clustering approach from the RAPTOR paper. Each logical part of the algorithm is defined in its own cell for maximum clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component-umap",
   "metadata": {},
   "source": [
    "#### Component 1: Dimensionality Reduction with UMAP\n",
    "\n",
    "**What it is:** UMAP (Uniform Manifold Approximation and Projection) is a technique for reducing the number of dimensions in our data.\n",
    "\n",
    "**Why we need it:** Text embeddings exist in a very high-dimensional space (e.g., 384 dimensions for our model). This can make it difficult for clustering algorithms to work effectively due to the \"Curse of Dimensionality.\" UMAP creates a lower-dimensional \"map\" of the data that preserves the essential semantic relationships, making it much easier to identify meaningful clusters.\n",
    "\n",
    "**How it works:** We define two functions: `global_cluster_embeddings` for a broad, initial reduction, and `local_cluster_embeddings` for a more fine-grained reduction within already identified clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-clustering-code-1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality reduction functions defined.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "RANDOM_SEED = 224  # for reproducibility\n",
    "\n",
    "def global_cluster_embeddings(embeddings: np.ndarray, dim: int, n_neighbors: Optional[int] = None, metric: str = \"cosine\") -> np.ndarray:\n",
    "    \"\"\"Perform global dimensionality reduction on the embeddings using UMAP.\"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric, random_state=RANDOM_SEED\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "def local_cluster_embeddings(embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\") -> np.ndarray:\n",
    "    \"\"\"Perform local dimensionality reduction on the embeddings using UMAP.\"\"\"\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=num_neighbors, n_components=dim, metric=metric, random_state=RANDOM_SEED\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "print(\"Dimensionality reduction functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component-bic",
   "metadata": {},
   "source": [
    "#### Component 2: Optimal Cluster Number Detection\n",
    "\n",
    "**What it is:** A function to automatically determine the best number of clusters for a given set of data points.\n",
    "\n",
    "**Why we need it:** Manually setting the number of clusters (`k`) is inefficient and often incorrect. A data-driven approach is far more robust. This function tests a range of possible cluster numbers and selects the one that best fits the data's structure.\n",
    "\n",
    "**How it works:** It uses a Gaussian Mixture Model (GMM) and evaluates each potential number of clusters using the **Bayesian Information Criterion (BIC)**. The BIC is a statistical measure that rewards models for goodness-of-fit while penalizing them for complexity (too many clusters). The number of clusters that results in the lowest BIC score is chosen as the optimal one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bic-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal cluster detection function defined.\n"
     ]
    }
   ],
   "source": [
    "def get_optimal_clusters(embeddings: np.ndarray, max_clusters: int = 50) -> int:\n",
    "    \"\"\"Determine the optimal number of clusters using the Bayesian Information Criterion (BIC).\"\"\"\n",
    "    # Limit the max number of clusters to be less than the number of data points\n",
    "    max_clusters = min(max_clusters, len(embeddings))\n",
    "    if max_clusters <= 1: \n",
    "        return 1\n",
    "    \n",
    "    # Test different numbers of clusters\n",
    "    n_clusters_range = np.arange(1, max_clusters)\n",
    "    bics = []\n",
    "    for n in n_clusters_range:\n",
    "        gmm = GaussianMixture(n_components=n, random_state=RANDOM_SEED)\n",
    "        gmm.fit(embeddings)\n",
    "        bics.append(gmm.bic(embeddings)) # Calculate BIC for the current model\n",
    "        \n",
    "    # Return the number of clusters that had the lowest BIC score\n",
    "    return n_clusters_range[np.argmin(bics)]\n",
    "\n",
    "print(\"Optimal cluster detection function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component-gmm",
   "metadata": {},
   "source": [
    "#### Component 3: Probabilistic Clustering with GMM\n",
    "\n",
    "**What it is:** A function that clusters the data and assigns labels based on probability.\n",
    "\n",
    "**Why we need it:** Unlike simpler algorithms like K-Means which assign each point to exactly one cluster (hard clustering), GMM is a probabilistic model (soft clustering). It calculates the *probability* that a data point belongs to each cluster. This is powerful for text, as a single document chunk might be relevant to multiple topics. By using a probability `threshold`, we can assign a chunk to all clusters for which its membership probability is sufficiently high.\n",
    "\n",
    "**How it works:** It first calls `get_optimal_clusters` to find the best `n_components`. It then fits a GMM and uses `predict_proba` to get the membership probabilities. Finally, it applies the `threshold` to assign the final cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gmm-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilistic clustering function defined.\n"
     ]
    }
   ],
   "source": [
    "def GMM_cluster(embeddings: np.ndarray, threshold: float) -> Tuple[List[np.ndarray], int]:\n",
    "    \"\"\"Cluster embeddings using a GMM and a probability threshold.\"\"\"\n",
    "    # Find the optimal number of clusters for this set of embeddings\n",
    "    n_clusters = get_optimal_clusters(embeddings)\n",
    "    \n",
    "    # Fit the GMM with the optimal number of clusters\n",
    "    gmm = GaussianMixture(n_components=n_clusters, random_state=RANDOM_SEED)\n",
    "    gmm.fit(embeddings)\n",
    "    \n",
    "    # Get the probability of each point belonging to each cluster\n",
    "    probs = gmm.predict_proba(embeddings)\n",
    "    \n",
    "    # Assign a point to a cluster if its probability is above the threshold\n",
    "    # A single point can be assigned to multiple clusters.\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    \n",
    "    return labels, n_clusters\n",
    "\n",
    "print(\"Probabilistic clustering function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component-orchestrator",
   "metadata": {},
   "source": [
    "#### Component 4: Hierarchical Clustering Orchestrator\n",
    "\n",
    "**What it is:** The main clustering function that ties all the previous components together to perform a multi-stage, hierarchical clustering.\n",
    "\n",
    "**Why we need it:** A single layer of clustering might not be enough. This function implements the paper's strategy of finding both broad themes and specific sub-topics.\n",
    "\n",
    "**How it works:**\n",
    "1.  **Global Stage:** It first runs UMAP and GMM on the *entire* dataset to find broad, high-level clusters (e.g., \"Transformers Library\", \"Datasets Library\").\n",
    "2.  **Local Stage:** It then iterates through each of these global clusters. For each one, it takes only the documents belonging to it and runs *another* round of UMAP and GMM. This finds finer-grained sub-topics (e.g., within \"Transformers Library\", it might find clusters for \"Pipelines\", \"Training\", and \"Models\").\n",
    "3.  **Label Aggregation:** It carefully combines the local cluster labels into a final, comprehensive list of cluster assignments for every document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orchestrator-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchical clustering orchestrator defined.\n"
     ]
    }
   ],
   "source": [
    "def perform_clustering(embeddings: np.ndarray, dim: int = 10, threshold: float = 0.1) -> List[np.ndarray]:\n",
    "    \"\"\"Perform hierarchical clustering (global and local) on the embeddings.\"\"\"\n",
    "    # Handle cases with very few documents to avoid errors\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "\n",
    "    # --- Global Clustering Stage ---\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    global_clusters, n_global_clusters = GMM_cluster(reduced_embeddings_global, threshold)\n",
    "\n",
    "    # --- Local Clustering Stage ---\n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    "\n",
    "    # Iterate through each global cluster to find sub-clusters\n",
    "    for i in range(n_global_clusters):\n",
    "        # Get all original indices for embeddings in the current global cluster\n",
    "        global_cluster_indices = [idx for idx, gc in enumerate(global_clusters) if i in gc]\n",
    "        if not global_cluster_indices:\n",
    "            continue\n",
    "        \n",
    "        # Get the actual embeddings for this global cluster\n",
    "        global_cluster_embeddings_ = embeddings[global_cluster_indices]\n",
    "\n",
    "        # Perform local clustering on this subset of embeddings\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            local_clusters, n_local_clusters = ([np.array([0])] * len(global_cluster_embeddings_)), 1\n",
    "        else:\n",
    "            reduced_embeddings_local = local_cluster_embeddings(global_cluster_embeddings_, dim)\n",
    "            local_clusters, n_local_clusters = GMM_cluster(reduced_embeddings_local, threshold)\n",
    "\n",
    "        # Map the local cluster results back to the original document indices\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_indices = [idx for idx, lc in enumerate(local_clusters) if j in lc]\n",
    "            if not local_cluster_indices:\n",
    "                continue\n",
    "            \n",
    "            original_indices = [global_cluster_indices[idx] for idx in local_cluster_indices]\n",
    "            for idx in original_indices:\n",
    "                all_local_clusters[idx] = np.append(all_local_clusters[idx], j + total_clusters)\n",
    "\n",
    "        total_clusters += n_local_clusters\n",
    "\n",
    "    return all_local_clusters\n",
    "\n",
    "print(\"Hierarchical clustering orchestrator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tree-builder-theory",
   "metadata": {},
   "source": [
    "### Step 5: Building the Tree\n",
    "\n",
    "With all the clustering components defined, we now create the functions that will recursively build the tree. This involves two final components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component-summarizer",
   "metadata": {},
   "source": [
    "#### Component 5: The Abstraction Engine (Summarization)\n",
    "\n",
    "**What it is:** A function that takes all the text from a single cluster and uses an LLM to generate a single, high-quality summary.\n",
    "\n",
    "**Why we need it:** This is the \"A\" in RAPTOR - **Abstractive**. This step doesn't just extract information; it *creates* new, higher-level knowledge. The summary of a cluster becomes a parent node in our tree, representing the distilled essence of all its child documents. This is how we move up the ladder of abstraction.\n",
    "\n",
    "**How it works:** We create a LangChain Expression Language (LCEL) chain with a detailed prompt that instructs the LLM to act as an expert technical writer and synthesize the provided context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summarizer-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization engine defined.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define the summarization chain\n",
    "summarization_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an expert technical writer. \n",
    "    Given the following collection of text chunks from the Hugging Face documentation, synthesize them into a single, coherent, and detailed summary. \n",
    "    Focus on the main concepts, APIs, and workflows described.\n",
    "    CONTEXT: {context}\n",
    "    DETAILED SUMMARY:\"\"\"\n",
    ")\n",
    "summarization_chain = summarization_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"Summarization engine defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component-recursion",
   "metadata": {},
   "source": [
    "#### Component 6: The Recursive Tree Builder\n",
    "\n",
    "**What it is:** The main recursive function that orchestrates the entire tree-building process, level by level.\n",
    "\n",
    "**Why we need it:** This function automates the hierarchical construction. It ensures that the process of clustering and summarizing is repeated on the outputs of the previous level, creating the layered structure of the RAPTOR index.\n",
    "\n",
    "**How it works:**\n",
    "1.  It takes a list of texts for the current `level`.\n",
    "2.  It calls `perform_clustering` and the `summarization_chain` to process this level.\n",
    "3.  It checks if the stopping conditions are met (max levels reached, or only one cluster was found).\n",
    "4.  If not, it **calls itself** with the newly generated summaries as the input for the next level (`level + 1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recursive-tree-builder",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursive tree builder defined.\n"
     ]
    }
   ],
   "source": [
    "def recursive_build_tree(texts: List[str], level: int = 1, n_levels: int = 3) -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"The main recursive function to build the RAPTOR tree using all components.\"\"\"\n",
    "    results = {}\n",
    "    # Base case: stop if max level is reached or no texts to process\n",
    "    if level > n_levels or len(texts) <= 1:\n",
    "        return results\n",
    "\n",
    "    # --- Embed and Cluster ---\n",
    "    text_embeddings_np = np.array(embeddings.embed_documents(texts))\n",
    "    cluster_labels = perform_clustering(text_embeddings_np)\n",
    "    df_clusters = pd.DataFrame({'text': texts, 'cluster': cluster_labels})\n",
    "\n",
    "    # --- Prepare for Summarization by expanding clusters ---\n",
    "    expanded_list = []\n",
    "    for _, row in df_clusters.iterrows():\n",
    "        for cluster_id in row['cluster']:\n",
    "            expanded_list.append({'text': row['text'], 'cluster': int(cluster_id)})\n",
    "    \n",
    "    if not expanded_list:\n",
    "        return results\n",
    "        \n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    "    all_clusters = expanded_df['cluster'].unique()\n",
    "    print(f\"--- Level {level}: Generated {len(all_clusters)} clusters ---\")\n",
    "\n",
    "    # --- Summarize each cluster ---\n",
    "    summaries = []\n",
    "    for i in all_clusters:\n",
    "        cluster_texts = expanded_df[expanded_df['cluster'] == i]['text'].tolist()\n",
    "        formatted_txt = \"\\n\\n---\\n\\n\".join(cluster_texts)\n",
    "        summary = summarization_chain.invoke({\"context\": formatted_txt})\n",
    "        summaries.append(summary)\n",
    "        print(f\"Level {level}, Cluster {i}: Generated summary of length {len(summary)} chars.\")\n",
    "\n",
    "    df_summary = pd.DataFrame({'summaries': summaries, 'cluster': all_clusters})\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # --- Recurse if possible ---\n",
    "    if level < n_levels and len(all_clusters) > 1:\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_build_tree(new_texts, level + 1, n_levels)\n",
    "        results.update(next_level_results)\n",
    "\n",
    "    return results\n",
    "\n",
    "print(\"Recursive tree builder defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build-tree-exec-theory",
   "metadata": {},
   "source": [
    "#### Executing the Tree-Building Process\n",
    "\n",
    "Now, we execute the main recursive function on our initial leaf nodes. This will build the entire tree structure, generating summaries at each level. This is the most computationally intensive step of the entire notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-tree-code",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Level 1: Generated 8 clusters ---\n",
      "Level 1, Cluster 0: Generated summary of length 2011 chars.\n",
      "Level 1, Cluster 1: Generated summary of length 1954 chars.\n",
      "Level 1, Cluster 2: Generated summary of length 2089 chars.\n",
      "Level 1, Cluster 3: Generated summary of length 1877 chars.\n",
      "Level 1, Cluster 4: Generated summary of length 2043 chars.\n",
      "Level 1, Cluster 5: Generated summary of length 1998 chars.\n",
      "Level 1, Cluster 6: Generated summary of length 2015 chars.\n",
      "Level 1, Cluster 7: Generated summary of length 1932 chars.\n",
      "--- Level 2: Generated 3 clusters ---\n",
      "Level 2, Cluster 0: Generated summary of length 2050 chars.\n",
      "Level 2, Cluster 1: Generated summary of length 1988 chars.\n",
      "Level 2, Cluster 2: Generated summary of length 1965 chars.\n"
     ]
    }
   ],
   "source": [
    "# Execute the RAPTOR process on our chunked leaf_texts.\n",
    "# This will build a tree with a maximum of 3 levels of summarization.\n",
    "raptor_results = recursive_build_tree(leaf_texts, level=1, n_levels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collapsed-tree-theory",
   "metadata": {},
   "source": [
    "### Step 6: Indexing with the \"Collapsed Tree\" Strategy\n",
    "\n",
    "**What it is:** Instead of building a complex graph data structure, we use a simple and effective strategy called the \"collapsed tree.\" We create a single, unified list containing **all** the text from every level of the tree: the original leaf chunks and all the generated summaries.\n",
    "\n",
    "**Why we do it:** This allows us to use a standard vector store (like FAISS or Chroma) for retrieval. A single similarity search on this vector store will now query across all levels of abstraction simultaneously. It's an elegant simplification that works remarkably well.\n",
    "\n",
    "**How it works:** We iterate through our `raptor_results`, collect all the leaf texts and summaries into one list, and then build a FAISS vector store from this combined corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collapsed-tree-code",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built vector store with 423 total documents (leaves + summaries).\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Combine all texts (original chunks and all generated summaries) into a single list.\n",
    "all_texts = leaf_texts.copy()\n",
    "for level in raptor_results:\n",
    "    # Get the summaries from the current level's results\n",
    "    summaries = raptor_results[level][1]['summaries'].tolist()\n",
    "    # Add them to our master list\n",
    "    all_texts.extend(summaries)\n",
    "\n",
    "# Build the final vector store using FAISS, a fast in-memory vector database.\n",
    "vectorstore = FAISS.from_texts(texts=all_texts, embedding=embeddings)\n",
    "\n",
    "# Create a retriever from the vector store.\n",
    "# We configure it to retrieve the top 5 most similar documents for any query.\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 5})\n",
    "\n",
    "print(f\"Built vector store with {len(all_texts)} total documents (leaves + summaries).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-chain-theory",
   "metadata": {},
   "source": [
    "### Step 7: Retrieval and Generation (RAG)\n",
    "\n",
    "Finally, we construct a RAG chain to ask questions. The retriever will fetch the most relevant documents (chunks or summaries) from our RAPTOR index, and the LLM will generate a final answer based on that context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-chain-code",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain created. Ready for querying.\n"
     ]
    }
   ],
   "source": [
    "# This prompt template is for the final generation step.\n",
    "final_prompt_text = \"\"\"You are an expert assistant for the Hugging Face ecosystem. \n",
    "Answer the user's question based ONLY on the following context. If the context does not contain the answer, state that you don't know.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_template(final_prompt_text)\n",
    "\n",
    "# A helper function to format the retrieved documents into a single string.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Construct the final RAG chain using LCEL.\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} # Retrieve and format context\n",
    "    | final_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG chain created. Ready for querying.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "querying-theory",
   "metadata": {},
   "source": [
    "#### Querying the Multi-Resolution Index\n",
    "\n",
    "Now we demonstrate the power of the RAPTOR index by asking questions at different levels of abstraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query1-theory",
   "metadata": {},
   "source": [
    "##### Query 1: Specific, Low-Level Question\n",
    "\n",
    "This type of query should match a granular **leaf node** containing a specific API or code example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query1-code",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `pipeline` function is the easiest way to use a pre-trained model for a given task. You simply instantiate a pipeline by specifying the task you want to perform, and the library handles the loading of the appropriate model and tokenizer for you.\n",
      "\n",
      "Here is a simple code example for a sentiment analysis task:\n",
      "\n",
      "```python\n",
      "from transformers import pipeline\n",
      "\n",
      "# Create a sentiment analysis pipeline\n",
      "classifier = pipeline(\"sentiment-analysis\")\n",
      "\n",
      "# Use the pipeline on some text\n",
      "result = classifier(\"I love using Hugging Face libraries!\")\n",
      "print(result)\n",
      "# Output: [{'label': 'POSITIVE', 'score': 0.9998}]\n",
      "```\n",
      "\n",
      "You can use it for many other tasks like \"text-generation\", \"question-answering\", and \"summarization\" by changing the task name.\n"
     ]
    }
   ],
   "source": [
    "question_specific = \"How do I use the `pipeline` function in the Transformers library? Give me a simple code example.\"\n",
    "answer = rag_chain.invoke(question_specific)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query2-theory",
   "metadata": {},
   "source": [
    "##### Query 2: Mid-Level, Conceptual Question\n",
    "\n",
    "This query asks about a process or workflow. It is likely to match one of the **generated summaries** from Level 1 or 2, which synthesizes information from multiple detailed chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query2-code",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning a model using the Parameter-Efficient Fine-Tuning (PEFT) library involves several key steps to efficiently adapt a large pre-trained model to a new task without modifying all of its parameters:\n",
      "\n",
      "1.  **Load a Base Model:** Start by loading your large pre-trained model from the Transformers library (e.g., a model from the `AutoModelForCausalLM` class).\n",
      "\n",
      "2.  **Create a PEFT Config:** Define a configuration for the PEFT method you want to use. For example, for LoRA (Low-Rank Adaptation), you would create a `LoraConfig` where you specify parameters like the rank (`r`), alpha (`lora_alpha`), and the target modules.\n",
      "\n",
      "3.  **Wrap the Model:** Use the `get_peft_model` function to wrap your base model with the PEFT configuration. This freezes the original weights and inserts the small, trainable adapter layers.\n",
      "\n",
      "4.  **Train the Model:** Proceed with training as you normally would using the Transformers `Trainer` or your own custom training loop. Only the adapter weights will be updated, making this process much faster and more memory-efficient.\n",
      "\n",
      "5.  **Save and Load:** After training, you can save the trained adapter weights, which are very small. To use the fine-tuned model, you load the base model and then apply the saved adapter weights to it.\n"
     ]
    }
   ],
   "source": [
    "question_mid_level = \"What are the main steps involved in fine-tuning a model using the PEFT library?\"\n",
    "answer = rag_chain.invoke(question_mid_level)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query3-theory",
   "metadata": {},
   "source": [
    "##### Query 3: Broad, High-Level Question\n",
    "\n",
    "This is the type of query where standard RAG fails. It should match a **high-level summary** near the top of our RAPTOR tree, providing a concise, thematic overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query3-code",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the core philosophy of the Hugging Face ecosystem is to democratize state-of-the-art natural language processing and machine learning. This is achieved through a set of interoperable, open-source libraries built on three main principles:\n",
      "\n",
      "1.  **Accessibility and Ease of Use:** Libraries like `transformers` with its `pipeline` function are designed to make it incredibly simple for users to access and use powerful pre-trained models with just a few lines of code.\n",
      "\n",
      "2.  **Modularity and Interoperability:** The ecosystem is designed as a modular stack. `datasets` handles data loading and processing, `tokenizers` provides fast and versatile tokenization, `transformers` offers the core models, and `accelerate` simplifies scaling training to any infrastructure. These libraries work seamlessly together.\n",
      "\n",
      "3.  **Efficiency and Performance:** While being easy to use, the ecosystem is built for performance. Techniques like Parameter-Efficient Fine-Tuning (PEFT) and tools like `accelerate` allow users to train and deploy large models efficiently, reducing computational costs and memory requirements. The underlying code is optimized for both research and production environments.\n"
     ]
    }
   ],
   "source": [
    "question_high_level = \"What is the core philosophy of the Hugging Face ecosystem?\"\n",
    "answer = rag_chain.invoke(question_high_level)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This notebook demonstrated the end-to-end implementation of the RAPTOR methodology using an advanced, multi-stage clustering approach. By breaking down each component and explaining its role, we have built a powerful, multi-resolution index from scratch. The final RAG system was able to effectively answer questions at various levels of abstraction, from specific code details to high-level strategic concepts, overcoming the primary limitation of standard RAG systems."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
